------------------------------------------------------------------------------------------------------
ATELIER PRA/PCA
------------------------------------------------------------------------------------------------------
L‚Äôid√©e en 30 secondes : Cet atelier met en ≈ìuvre un **mini-PRA** sur **Kubernetes** en d√©ployant une **application Flask** avec une **base SQLite** stock√©e sur un **volume persistant (PVC pra-data)** et des **sauvegardes automatiques r√©alis√©es chaque minute vers un second volume (PVC pra-backup)** via un **CronJob**. L‚Äô**image applicative est construite avec Packer** et le **d√©ploiement orchestr√© avec Ansible**, tandis que Kubernetes assure la gestion des pods et de la disponibilit√© applicative. Nous observerons la diff√©rence entre **disponibilit√©** (recr√©ation automatique des pods sans perte de donn√©es) et **reprise apr√®s sinistre** (perte volontaire du volume de donn√©es puis restauration depuis les backups), nous mesurerons concr√®tement les RTO et RPO, et comprendrons les limites d‚Äôun PRA local non r√©pliqu√©. Cet atelier illustre de mani√®re pratique les principes de continuit√© et de reprise d‚Äôactivit√©, ainsi que le r√¥le respectif des conteneurs, du stockage persistant et des m√©canismes de sauvegarde.
  
**Architecture cible :** Ci-dessous, voici l'architecture cible souhait√©e.   
  
![Screenshot Actions](Architecture_cible.png)  
  
-------------------------------------------------------------------------------------------------------
S√©quence 1 : Codespace de Github
-------------------------------------------------------------------------------------------------------
Objectif : Cr√©ation d'un Codespace Github  
Difficult√© : Tr√®s facile (~5 minutes)
-------------------------------------------------------------------------------------------------------
**Faites un Fork de ce projet**. Si besoin, voici une vid√©o d'accompagnement pour vous aider √† "Forker" un Repository Github : [Forker ce projet](https://youtu.be/p33-7XQ29zQ) 
  
Ensuite depuis l'onglet **[CODE]** de votre nouveau Repository, **ouvrez un Codespace Github**.
  
---------------------------------------------------
S√©quence 2 : Cr√©ation du votre environnement de travail
---------------------------------------------------
Objectif : Cr√©er votre environnement de travail  
Difficult√© : Simple (~10 minutes)
---------------------------------------------------
Vous allez dans cette s√©quence mettre en place un cluster Kubernetes K3d contenant un master et 2 workers, installer les logiciels Packer et Ansible. Depuis le terminal de votre Codespace copier/coller les codes ci-dessous √©tape par √©tape :  

**Cr√©ation du cluster K3d**  
```
curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
```
```
k3d cluster create pra \
  --servers 1 \
  --agents 2
```
**v√©rification de la cr√©ation de votre cluster Kubernetes**  
```
kubectl get nodes
```
**Installation du logiciel Packer (cr√©ation d'images Docker)**  
```
PACKER_VERSION=1.11.2
curl -fsSL -o /tmp/packer.zip \
  "https://releases.hashicorp.com/packer/${PACKER_VERSION}/packer_${PACKER_VERSION}_linux_amd64.zip"
sudo unzip -o /tmp/packer.zip -d /usr/local/bin
rm -f /tmp/packer.zip
```
**Installation du logiciel Ansible**  
```
python3 -m pip install --user ansible kubernetes PyYAML jinja2
export PATH="$HOME/.local/bin:$PATH"
ansible-galaxy collection install kubernetes.core
```
  
---------------------------------------------------
S√©quence 3 : D√©ploiement de l'infrastructure
---------------------------------------------------
Objectif : D√©ployer l'infrastructure sur le cluster Kubernetes
Difficult√© : Facile (~15 minutes)
---------------------------------------------------  
Nous allons √† pr√©sent d√©ployer notre infrastructure sur Kubernetes. C'est √† dire, cr√©√©r l'image Docker de notre application Flask avec Packer, d√©poser l'image dans le cluster Kubernetes et enfin d√©ployer l'infratructure avec Ansible (Cr√©ation du pod, cr√©ation des PVC et les scripts des sauvegardes aututomatiques).  

**Cr√©ation de l'image Docker avec Packer**  
```
packer init .
packer build -var "image_tag=1.0" .
docker images | head
```
  
**Import de l'image Docker dans le cluster Kubernetes**  
```
k3d image import pra/flask-sqlite:1.0 -c pra
```
  
**D√©ploiment de l'infrastructure dans Kubernetes**  
```
ansible-playbook ansible/playbook.yml
```
  
**Forward du port 8080 qui est le port d'exposition de votre application Flask**  
```
kubectl -n pra port-forward svc/flask 8080:80 >/tmp/web.log 2>&1 &
```
  
---------------------------------------------------  
**R√©ccup√©ration de l'URL de votre application Flask**. Votre application Flask est d√©ploy√©e sur le cluster K3d. Pour obtenir votre URL cliquez sur l'onglet **[PORTS]** dans votre Codespace (√† cot√© de Terminal) et rendez public votre port 8080 (Visibilit√© du port). Ouvrez l'URL dans votre navigateur et c'est termin√©.  

**Les routes** √† votre disposition sont les suivantes :  
1. https://...**/** affichera dans votre navigateur "Bonjour tout le monde !".
2. https://...**/health** pour voir l'√©tat de sant√© de votre application.
3. https://...**/add?message=test** pour ajouter un message dans votre base de donn√©es SQLite.
4. https://...**/count** pour afficher le nombre de messages stock√©s dans votre base de donn√©es SQLite.
5. https://...**/consultation** pour afficher les messages stock√©s dans votre base de donn√©es.
  
---------------------------------------------------  
### Processus de sauvegarde de la BDD SQLite

Gr√¢ce √† une t√¢che CRON d√©ploy√©e par Ansible sur le cluster Kubernetes (un CronJob), toutes les minutes une sauvegarde de la BDD SQLite est faite depuis le PVC pra-data vers le PCV pra-backup dans Kubernetes.  

Pour visualiser les sauvegardes p√©riodiques d√©pos√©es dans le PVC pra-backup, coller les commandes suivantes dans votre terminal Codespace :  

```
kubectl -n pra run debug-backup \
  --rm -it \
  --image=alpine \
  --overrides='
{
  "spec": {
    "containers": [{
      "name": "debug",
      "image": "alpine",
      "command": ["sh"],
      "stdin": true,
      "tty": true,
      "volumeMounts": [{
        "name": "backup",
        "mountPath": "/backup"
      }]
    }],
    "volumes": [{
      "name": "backup",
      "persistentVolumeClaim": {
        "claimName": "pra-backup"
      }
    }]
  }
}'
```
```
ls -lh /backup
```
**Pour sortir du cluster et revenir dans le terminal**
```
exit
```

---------------------------------------------------
S√©quence 4 : üí• Sc√©narios de crash possibles  
Difficult√© : Facile (~30 minutes)
---------------------------------------------------
### üé¨ **Sc√©nario 1 : PCA ‚Äî Crash du pod**  
Nous allons dans ce sc√©nario **d√©truire notre Pod Kubernetes**. Ceci simulera par exemple la supression d'un pod accidentellement, ou un pod qui crash, ou un pod red√©marr√©, etc..

**Destruction du pod :** Ci-dessous, la cible de notre sc√©nario   
  
![Screenshot Actions](scenario1.png)  

Nous perdons donc ici notre application mais pas notre base de donn√©es puisque celle-ci est d√©pos√©e dans le PVC pra-data hors du pod.  

Copier/coller le code suivant dans votre terminal Codespace pour d√©truire votre pod :
```
kubectl -n pra get pods
```
Notez le nom de votre pod qui est diff√©rent pour tout le monde.  
Supprimez votre pod (pensez √† remplacer <nom-du-pod-flask> par le nom de votre pod).  
Exemple : kubectl -n pra delete pod flask-7c4fd76955-abcde  
```
kubectl -n pra delete pod <nom-du-pod-flask>
```
**V√©rification de la suppression de votre pod**
```
kubectl -n pra get pods
```
üëâ **Le pod a √©t√© reconstruit sous un autre identifiant**.  
Forward du port 8080 du nouveau service  
```
kubectl -n pra port-forward svc/flask 8080:80 >/tmp/web.log 2>&1 &
```
Observez le r√©sultat en ligne  
https://...**/consultation** -> Vous n'avez perdu aucun message.
  
üëâ Kubernetes g√®re tout seul : Aucun impact sur les donn√©es ou sur votre service (PVC conserve la DB et le pod est reconstruit automatiquement) -> **C'est du PCA**. Tout est automatique et il n'y a aucune rupture de service.
  
---------------------------------------------------
### üé¨ **Sc√©nario 2 : PRA - Perte du PVC pra-data** 
Nous allons dans ce sc√©nario **d√©truire notre PVC pra-data**. C'est √† dire nous allons suprimer la base de donn√©es en production. Ceci simulera par exemple la corruption de la BDD SQLite, le disque du node perdu, une erreur humaine, etc. üí• Impact : IL s'agit ici d'un impact important puisque **la BDD est perdue**.  

**Destruction du PVC pra-data :** Ci-dessous, la cible de notre sc√©nario   
  
![Screenshot Actions](scenario2.png)  

üî• **PHASE 1 ‚Äî Simuler le sinistre (perte de la BDD de production)**  
Copier/coller le code suivant dans votre terminal Codespace pour d√©truire votre base de donn√©es :
```
kubectl -n pra scale deployment flask --replicas=0
```
```
kubectl -n pra patch cronjob sqlite-backup -p '{"spec":{"suspend":true}}'
```
```
kubectl -n pra delete job --all
```
```
kubectl -n pra delete pvc pra-data
```
üëâ Vous pouvez v√©rifier votre application en ligne, la base de donn√©es est d√©truite et la service n'est plus acc√©ssible.  

‚úÖ **PHASE 2 ‚Äî Proc√©dure de restauration**  
Recr√©er l‚Äôinfrastructure avec un PVC pra-data vide.  
```
kubectl apply -f k8s/
```
V√©rification de votre application en ligne.  
Forward du port 8080 du service pour tester l'application en ligne.  
```
kubectl -n pra port-forward svc/flask 8080:80 >/tmp/web.log 2>&1 &
```
https://...**/count** -> =0.  
https://...**/consultation** Vous avez perdu tous vos messages.  

Retaurez votre BDD depuis le PVC Backup.  
```
kubectl apply -f pra/50-job-restore.yaml
```
üëâ Vous pouvez v√©rifier votre application en ligne, **votre base de donn√©es a √©t√© restaure√©** et tous vos messages sont bien pr√©sents.  

Relance des CRON de sauvgardes.  
```
kubectl -n pra patch cronjob sqlite-backup -p '{"spec":{"suspend":false}}'
```
üëâ Nous n'avons pas perdu de donn√©es mais Kubernetes ne g√®re pas la restauration tout seul. Nous avons du prot√©ger nos donn√©es via des sauvegardes r√©guli√®res (du PVC pra-data vers le PVC pra-backup). -> **C'est du PRA**. Il s'agit d'une strat√©gie de sauvegarde avec une proc√©dure de restauration.  

---------------------------------------------------
S√©quence 5 : Exercices  
Difficult√© : Moyenne (~45 minutes)
---------------------------------------------------
**Compl√©tez et documentez ce fichier README.md** pour r√©pondre aux questions des exercices.  
Faites preuve de p√©dagogie et soyez clair dans vos explications et procedures de travail.  

**Exercice 1 :**  
Quels sont les composants dont la perte entra√Æne une perte de donn√©es ?  
  
*..R√©pondez √† cet exercice ici..*

**Exercice 2 :**  
Expliquez nous pourquoi nous n'avons pas perdu les donn√©es lors de la supression du PVC pra-data  
  
*..R√©pondez √† cet exercice ici..*

**Exercice 3 :**  
Quels sont les RTO et RPO de cette solution ?  
  
*..R√©pondez √† cet exercice ici..*

**Exercice 4 :**  
Pourquoi cette solution (cet atelier) ne peux pas √™tre utilis√© dans un vrai environnement de production ? Que manque-t-il ?   
  
*..R√©pondez √† cet exercice ici..*
  
**Exercice 5 :**  
Proposez une archtecture plus robuste.   
  
*..R√©pondez √† cet exercice ici..*

---------------------------------------------------
S√©quence 6 : Ateliers  
Difficult√© : Moyenne (~2 heures)
---------------------------------------------------
### **Atelier 1 : Ajoutez une fonctionnalit√© √† votre application**  
**Ajouter une route GET /status** dans votre application qui affiche en JSON :
* count : nombre d‚Äô√©v√©nements en base
* last_backup_file : nom du dernier backup pr√©sent dans /backup
* backup_age_seconds : √¢ge du dernier backup

*..**D√©posez ici une copie d'√©cran** de votre r√©ussite..*

---------------------------------------------------
### **Atelier 2 : Choisir notre point de restauration**  
Aujourd‚Äôhui nous restaurobs ‚Äúle dernier backup‚Äù. Nous souhaitons **ajouter la capacit√© de choisir un point de restauration**.

*..D√©crir ici votre proc√©dure de restauration (votre runbook)..*  
  
---------------------------------------------------
Evaluation
---------------------------------------------------
Cet atelier PRA PCA, **not√© sur 20 points**, est √©valu√© sur la base du bar√®me suivant :  
- S√©rie d'exerices (5 points)
- Atelier N¬∞1 - Ajout d'un fonctionnalit√© (4 points)
- Atelier N¬∞2 - Choisir son point de restauration (4 points)
- Qualit√© du Readme (lisibilit√©, erreur, ...) (3 points)
- Processus travail (quantit√© de commits, coh√©rence globale, interventions externes, ...) (4 points) 



## S√©quence 5 : Exercices

### Exercice 1 : Quels sont les composants dont la perte entra√Æne une perte de donn√©es ?

La perte de donn√©es survient uniquement si les **deux volumes persistants sont perdus simultan√©ment** :

| Composant | Impact si perdu seul | Impact si perdu avec l'autre |
|---|---|---|
| **Pod Flask** | ‚ùå Aucune perte (stateless) | ‚ùå Aucune perte |
| **PVC pra-data** | ‚ö†Ô∏è Perte des donn√©es non encore sauvegard√©es (< 1 min) | üí• Perte totale |
| **PVC pra-backup** | ‚ö†Ô∏è Perte de l'historique des backups | üí• Perte totale |

Le Pod Flask est **stateless** : il ne stocke aucune donn√©e en lui-m√™me.
Toute la donn√©e applicative r√©side dans le **PVC pra-data** (BDD en production).
Le **PVC pra-backup** contient les sauvegardes permettant la restauration.

> ‚ö†Ô∏è Point critique : les deux PVC r√©sident sur le **m√™me disque physique du node K3d**.
> Si ce disque est perdu, les deux PVC sont d√©truits simultan√©ment ‚Üí perte totale des donn√©es.

---

### Exercice 2 : Pourquoi nous n'avons pas perdu les donn√©es lors de la suppression du pod (Sc√©nario 1 - PCA) ?

Dans Kubernetes, le **stockage est d√©coupl√© du cycle de vie du pod**.

Un PVC (PersistentVolumeClaim) est un objet Kubernetes **ind√©pendant** du pod.
Lorsque le pod Flask est supprim√© :
1. Kubernetes d√©tecte via le **Deployment** que le nombre de replicas souhait√© (1) n'est plus atteint
2. Il recr√©e automatiquement un **nouveau pod** sous un nouvel identifiant
3. Ce nouveau pod monte le **m√™me PVC pra-data**, qui n'a jamais √©t√© touch√©
4. La base SQLite est retrouv√©e intacte, aucune donn√©e n'est perdue

C'est la d√©finition du **PCA (Plan de Continuit√© d'Activit√©)** :
> La disponibilit√© est assur√©e **automatiquement et sans intervention humaine**.
> Il n'y a aucune rupture de service et aucune perte de donn√©es.

---

### Exercice 3 : Quels sont les RTO et RPO de cette solution ?

#### RPO ‚Äî Recovery Point Objective (Perte de donn√©es maximale acceptable)

> **RPO ‚âà 1 minute**

Le CronJob de sauvegarde s'ex√©cute **toutes les minutes**.
Dans le pire des cas, si un sinistre survient juste apr√®s un backup,
on perd au maximum **60 secondes** de donn√©es (les √©critures non encore sauvegard√©es).

#### RTO ‚Äî Recovery Time Objective (Dur√©e de restauration maximale acceptable)

> **RTO ‚âà 5 √† 15 minutes** (manuel)

Le RTO se d√©compose ainsi :

| √âtape | Dur√©e estim√©e |
|---|---|
| D√©tection du sinistre | ~1-2 min |
| Suppression du pod et PVC corrompu | ~1 min |
| Recr√©ation de l'infra (`kubectl apply`) | ~1-2 min |
| Lancement du job de restauration | ~1-2 min |
| V√©rification et remise en service | ~1-2 min |
| **Total** | **~5 √† 15 min** |

> ‚ö†Ô∏è Ce RTO est **manuel** et donc variable selon la disponibilit√© de l'op√©rateur.
> Il n'existe aucune automatisation de la proc√©dure de reprise dans cet atelier.

---

### Exercice 4 : Pourquoi cette solution ne peut pas √™tre utilis√©e en production ?

Cette architecture pr√©sente plusieurs **limitations critiques** qui la rendent inadapt√©e √† un vrai environnement de production :

#### üî¥ Probl√®mes de r√©silience
- **Single Point of Failure** : les deux PVC sont sur le **m√™me disque du node K3d**.
  Un crash du node entra√Æne la perte simultan√©e des donn√©es ET des backups.
- **Pas de r√©plication** : aucune copie des donn√©es dans un second datacenter ou zone de disponibilit√©.
  Un sinistre physique (incendie, inondation, panne mat√©rielle) d√©truit tout.
- **Cluster K3d mono-node effectif** : K3d tourne dans un Codespace √©ph√©m√®re,
  si le Codespace est d√©truit, le cluster entier dispara√Æt.

#### üî¥ Probl√®mes de s√©curit√©
- **Backups non chiffr√©s** : les fichiers `.db` sont copi√©s en clair dans `/backup`.
- **Pas de contr√¥le d'acc√®s** sur les volumes (RBAC insuffisant).
- **SQLite** n'est pas con√ßu pour un usage en production multi-utilisateurs
  (pas de connexions concurrentes, pas de haute disponibilit√© native).

#### üî¥ Probl√®mes op√©rationnels
- **Restauration 100% manuelle** : aucun runbook automatis√©, d√©pendance √† l'humain.
- **Pas de monitoring** : aucune alerte si le CronJob √©choue ou si un backup est corrompu.
- **Pas de r√©tention** : les backups s'accumulent ind√©finiment, sans rotation ni purge.
- **RPO de 1 minute** peut √™tre insuffisant pour des donn√©es critiques (transactions bancaires, etc.).

---

### Exercice 5 : Proposition d'une architecture plus robuste

#### Am√©liorations propos√©es

**1. Base de donn√©es production-ready**
- Remplacer SQLite par **PostgreSQL** g√©r√© par un op√©rateur Kubernetes
  (ex : [CloudNativePG](https://cloudnative-pg.io/)) avec r√©plication synchrone master/replica.

**2. Stockage r√©pliqu√©**
- Utiliser un **StorageClass avec r√©plication** entre nodes et zones :
  [Longhorn](https://longhorn.io/), [Rook/Ceph](https://rook.io/), ou le CSI natif du cloud provider.
- Activer les **VolumeSnapshots** CSI pour des snapshots instantan√©s et coh√©rents.

**3. Sauvegardes externes chiffr√©es**
- D√©ployer **[Velero](https://velero.io/)** pour sauvegarder namespaces + PVC
  vers un stockage objet externe (S3, Azure Blob, GCS) avec chiffrement AES-256.
- Politique de r√©tention : 7 jours de backups quotidiens, 4 semaines de backups hebdomadaires.

**4. Haute disponibilit√© du cluster**
- Cluster Kubernetes **multi-n≈ìuds** avec 3 masters (etcd en HA)
- D√©ploiement **multi-zone** voire **multi-r√©gion** pour le DR g√©ographique.
- `PodDisruptionBudgets` + `Liveness/Readiness probes` sur tous les pods critiques.

**5. Observabilit√© et automatisation**
- **Prometheus + Alertmanager** : alertes sur √©chec de CronJob, saturation du stockage.
- **Grafana** : tableaux de bord RTO/RPO en temps r√©el.
- **Runbook automatis√©** de restauration via ArgoCD ou un op√©rateur custom.
- **Game Days** r√©guliers pour valider les RTO/RPO r√©els en conditions de production.

#### Comparaison RTO/RPO

| Architecture | RPO | RTO |
|---|---|---|
| Atelier (K3d + CronJob 1min) | ~1 minute | ~5-15 min (manuel) |
| Production (PostgreSQL + Velero) | ~0 (r√©plication sync) | ~2-5 min (automatis√©) |
| Production multi-r√©gion | ~0 (r√©plication sync) | <1 min (bascule automatique) |
